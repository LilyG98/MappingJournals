{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Lily Grumbach\n",
    "    <br>M1 Humanités numériques - Université PSL</h4>\n",
    "<h1><center>Rendu TAIS-TAL </center></h1>\n",
    "<h2><center>Partie TAIS</center></h2>\n",
    "<h3><center>2/3 : Extraire des bases de données à ma disposition les coordonnées des lieux et créer un dictionnaire de désambiguisation de mes données</center></h3>\n",
    "\n",
    "<b><u>PLAN:</u></b>\n",
    "1) Harmonisation des données Géopolitiques pour le fond de carte de 1914\n",
    "<br>\n",
    "    \n",
    "<h4>2) Explorer les bases de données à ma disposition (IREL et GPH) et les préparer pour l'extraction des coordonnées géographiques </h4>\n",
    "<br/>\n",
    "    \n",
    "3) Géoréférencer les données de mes revues\n",
    "\n",
    "<b>Stratégie en place :  </b>\n",
    "\n",
    "Nous avons 3 sources possibles de désambiguisation : \n",
    "* **Wikidata** indiqués par le medialab \n",
    "* l'**IREL**, la base de données géographique de l'inventaire en ligne des Archives nationales d'Outre Mer\n",
    "* **Geonames**, pour le meilleur et pour le pire.\n",
    "\n",
    "<h3>Objectifs de la partie : </h3>\n",
    "Faire une estimation de ce que je pourrai récupérer avec les données bases de données que j'ai et les préparer pour l'extraction de coordonnées géographiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#module\n",
    "import desambiguisation\n",
    "\n",
    "##General : \n",
    "from desambiguisation import ListEntities2df\n",
    "##GPH\n",
    "from desambiguisation import MatchGPELOC_IREL,Extract_longlat_WikiData_ALL\n",
    "##IREL : \n",
    "from desambiguisation import nettoyage_df_IREL,urlencode,Extract_longlat_IREL,IREL_Nettoyage_AdminLieuDit,nettoyage_desambiguisation\n",
    "\n",
    "#Généraux\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#Webscrapping\n",
    "from bs4 import BeautifulSoup\n",
    "import geocoder\n",
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "#Wikidata \n",
    "import qwikidata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Les df de référence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AHMC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fichier créé dans ./desambiguisation/Revue_ParEntityLabel/AHMC/AHMCGPE_ents.csv\n",
      "Output:liste des entités GPE_ents \n",
      "\n",
      "fichier créé dans ./desambiguisation/Revue_ParEntityLabel/AHMC/AHMCLOC_ents.csv\n",
      "Output:liste des entités LOC_ents \n",
      "\n",
      "fichier créé dans ./desambiguisation/Revue_ParEntityLabel/AHMC/AHMCORG_ents.csv\n",
      "Output:liste des entités ORG_ents \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##AHMC\n",
    "df_AHMC_annote = pd.read_csv('./df_annotes/dfAHMCannotations.csv')\n",
    "\n",
    "#Ne conserver qu'une partie du df\n",
    "df_AHMC_annote = df_AHMC_annote[[\"article_titre\",\"revue_annee\",\"GPE_ents\",\"LOC_ents\",\"ORG_ents\"]]\n",
    "\n",
    "#Extraire les entités GPE,LOC,ORG\n",
    "liste_GPEAHMC = ListEntities2df(df_AHMC_annote,\"AHMC\",\"GPE_ents\")\n",
    "liste_LOCAHMC = ListEntities2df(df_AHMC_annote,\"AHMC\",\"LOC_ents\")\n",
    "liste_ORGAHMC = ListEntities2df(df_AHMC_annote,\"AHMC\",\"ORG_ents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AMN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fichier créé dans ./desambiguisation/Revue_ParEntityLabel/AMN/AMNGPE_ents.csv\n",
      "Output:liste des entités GPE_ents \n",
      "\n",
      "fichier créé dans ./desambiguisation/Revue_ParEntityLabel/AMN/AMNLOC_ents.csv\n",
      "Output:liste des entités LOC_ents \n",
      "\n",
      "fichier créé dans ./desambiguisation/Revue_ParEntityLabel/AMN/AMNORG_ents.csv\n",
      "Output:liste des entités ORG_ents \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#AMN\n",
    "df_AMN_annote = pd.read_csv('./df_annotes/dfAMNannotations.csv')\n",
    "\n",
    "#Ne conserver qu'une partie du df\n",
    "df_AMN_annote = df_AMN_annote[[\"article_titre\",\"revue_annee\",\"GPE_ents\",\"LOC_ents\",\"ORG_ents\"]]\n",
    "\n",
    "#Extraire les entités GPE,LOC,ORG\n",
    "liste_GPEAMN = ListEntities2df(df_AMN_annote,\"AMN\",\"GPE_ents\")\n",
    "liste_LOCAMN = ListEntities2df(df_AMN_annote,\"AMN\",\"LOC_ents\")\n",
    "liste_ORGAMN = ListEntities2df(df_AMN_annote,\"AMN\",\"ORG_ents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Désambiguisation de la liste des GPE et LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lieux à annoter :  260\n"
     ]
    }
   ],
   "source": [
    "list_GPELOC = liste_GPEAMN+liste_LOCAMN+liste_LOCAHMC+liste_GPEAHMC\n",
    "print(\"Nombre de lieux à annoter : \",len(list_GPELOC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionnaire et liste des Faux positifs relevés dans la liste :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionnaire pour la désambiguisation :\n",
    "Dico_desambiguisation = {\"Guinée Française\":[\"Guinée française\"],\n",
    "                         \"Côte d'Ivoire\":[\"côte d'Ivoire\"],\n",
    "                         \"Pak-Hoi\":[\"Pak-Hoï\",\"Pakhoï\"],\n",
    "                         \"Chengdu\":['Tchen-Tou','Tchentou'],\n",
    "                         \"Yunnan\":['Yun-Nam','Yun-Nan','Yunnam'],\n",
    "                         \"Hanoi\":[\"Hanoï\"],\n",
    "                         \"Laokay\":[\"Lao-kay\"],\n",
    "                         \"Côte d'Ivoire\":[\"Côte d\\'Ivoire\",\"côte d'Ivoire\"]\n",
    "                        }\n",
    "Liste_FP = [\"Annamite\",\"Annamites\",\"île\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annamite\n",
      "Annamites\n",
      "île\n"
     ]
    }
   ],
   "source": [
    "#désambiguiser :\n",
    "for i in range(len(list_GPELOC)):\n",
    "    for (key,val) in Dico_desambiguisation.items():\n",
    "            for name in val:\n",
    "                if list_GPELOC[i]==name :\n",
    "                    list_GPELOC[i]=key\n",
    "\n",
    "#retirer lesfaux positifs :\n",
    "for FP in Liste_FP:\n",
    "    print(FP)\n",
    "    list_GPELOC.remove(FP)\n",
    "# list_GPELOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avant dédoublonnage :  257\n",
      "après dédoublonnage :  225\n"
     ]
    }
   ],
   "source": [
    "#Retirer les doublons de la liste :\n",
    "print(\"avant dédoublonnage : \",len(list_GPELOC))\n",
    "list_GPELOC = list(dict.fromkeys(list_GPELOC))\n",
    "print(\"après dédoublonnage : \",len(list_GPELOC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mise en df pour exploitation plus tard\n",
    "df_GPELOC = pd.DataFrame({\"NAME\":list_GPELOC})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taille finale des entités à annoter : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acores</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Algérie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Allemagne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Angleterre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Annam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>lac Tchad.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>île de Saint-Barthélémy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>île de Saint-Martin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>île de la Réunion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>îles Saint-Pierre</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        NAME\n",
       "0                     Acores\n",
       "1                    Algérie\n",
       "2                  Allemagne\n",
       "3                 Angleterre\n",
       "4                      Annam\n",
       "..                       ...\n",
       "220               lac Tchad.\n",
       "221  île de Saint-Barthélémy\n",
       "222      île de Saint-Martin\n",
       "223        île de la Réunion\n",
       "224        îles Saint-Pierre\n",
       "\n",
       "[225 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_GPELOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportation pour exploitation future : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GPELOC.to_csv(\"./output_finaux/GPELOC_à_annoter.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Explorer les possibilités entre GPH, IREL et GPELOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Importation et première exploration des BDD\n",
    "NB : je n'applique cela qu'aux BDD GPH et IREL car Geonames est beaucoup trop volumineux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. GPH \n",
    "Import de la base de données du medialab contenant les liens wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import du df GPH entities avec lien wikidata\n",
    "df_wikiData = pd.read_csv(\"./GeoPolHist-202103/medialab-GeoPolHist-fb19b66/data/GeoPolHist_entities.csv\")\n",
    "df_wikiData=df_wikiData.rename(columns={\"GPH_name\":\"NAME\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des données de 1914 nettoyées et enrichies manuellement pour correspondre au fond de carte de QGIS. On merge ce df avec celui contenant les liens wikidata :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importer le df de référence (jusqu'à 1914)\n",
    "df_MANUEL = pd.read_csv(\"./output_finaux/df_1914manuelCarto.csv\")\n",
    "\n",
    "#Merge avec les données complétées manuellement \n",
    "df_WikiQGIS=pd.merge(df_wikiData,df_MANUEL,how=\"right\",on=\"GPH_code\")\n",
    "\n",
    "#Export : \n",
    "df_WikiQGIS.to_csv(\"./output_finaux/WikiQGIS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1.2. IREL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nettoyer la base de donnée IREL (cf module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return: df_IREL nettoyé\n"
     ]
    }
   ],
   "source": [
    "dico_IREL ={}\n",
    "n=0\n",
    "\n",
    "with open('./IREL/liste_lieux_IREL.js','r') as f:\n",
    "    lignes = f.readlines()\n",
    "    \n",
    "    for l in lignes:\n",
    "        l=l.split(\"\\n\")\n",
    "        dico_IREL[n]=l\n",
    "        n+=1\n",
    "\n",
    "df_IREL = nettoyage_df_IREL(dico_IREL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Merge df_IREL et df_WikiData pour avoir les lieux présents dans les deux bases de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renommer les colonnes pour pouvoir merge :\n",
    "df_IREL=df_IREL.rename(columns={\"Lieu-dit\":\"NAME\"})\n",
    "df_wikiData=df_wikiData.rename(columns={\"GPH_name\":\"NAME\"})\n",
    "\n",
    "df_WikiIREL = pd.merge(df_wikiData,df_IREL,how=\"inner\",on=\"NAME\")\n",
    "\n",
    "#ATTENTION : je drop les duplicatas sur les GPH_code pour avoir une idée plus géographique qu'historique:\n",
    "df_WikiIREL[\"GPH_code\"]=df_WikiIREL[\"GPH_code\"].drop_duplicates(keep=\"first\")\n",
    "\n",
    "#Ici onsouhaite avoirleurnombredans l'absolu,regardless of the GPH Status\n",
    "df_WikiIREL=df_WikiIREL.loc[df_WikiIREL[\"GPH_code\"].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4.  Explorations de nos df :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> visualisation du nombre d'entité par df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df_IREL</th>\n",
       "      <th>df_wikiData</th>\n",
       "      <th>df_WikiQGIS</th>\n",
       "      <th>df_WikiIREL</th>\n",
       "      <th>df_GPELOC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Nombre d'entités</th>\n",
       "      <td>14571</td>\n",
       "      <td>1228</td>\n",
       "      <td>203</td>\n",
       "      <td>87</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  df_IREL  df_wikiData  df_WikiQGIS  df_WikiIREL  df_GPELOC\n",
       "Nombre d'entités    14571         1228          203           87        225"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comparaison_DF = pd.DataFrame({\"df_IREL\":len(df_IREL),\"df_wikiData\":len(df_wikiData),\"df_WikiQGIS\":len(df_WikiQGIS),\"df_WikiIREL\":len(df_WikiIREL),\"df_GPELOC\":len(df_GPELOC)},index=[\"Nombre d'entités\"])\n",
    "df_comparaison_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avoir une idée du nombre d'entités que je peux **à priori** (pour le meilleur comme pour le pire) trouver dans les deux bases de données : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GPELOC=df_GPELOC.rename(columns={\"entités à annoter\":\"NAME\"})\n",
    "\n",
    "#Nombre d'entités de GPELOC trouvable dans GPH: \n",
    "GPELOC_GPH =pd.merge(df_wikiData,df_GPELOC,how=\"inner\",on=\"NAME\")[[\"NAME\"]]\n",
    "\n",
    "#Nombre d'entités de GPELOC trouvable dans IREL: \n",
    "GPELOC_IREL =pd.merge(df_IREL,df_GPELOC,how=\"inner\",on=\"NAME\")[[\"NAME\"]]\n",
    "\n",
    "#Nombre d'entités de GPELOC dans IREL *et* das GPH : \n",
    "GPELOC_IRELGPH =pd.merge(df_WikiIREL,df_GPELOC,how=\"inner\",on=\"NAME\")[[\"NAME\"]]\n",
    "\n",
    "#Nombre d'entités de GPELOC dans GPH en 1914 :\n",
    "GPELOC_GPH1914 =pd.merge(df_WikiQGIS,df_GPELOC,how=\"inner\",on=\"NAME\")[[\"NAME\"]]\n",
    "\n",
    "df_comparaison_GPELOC = pd.DataFrame({\"avec GPH\":len(GPELOC_GPH),\"avec IREL\":len(GPELOC_IREL),\"IRELxGPH\":len(GPELOC_IRELGPH),\"GPH en1914\":len(GPELOC_GPH1914)},index=[\"Nombre d'entités\"])\n",
    "df_comparaison_GPELOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_ent_trouvables = GPELOC_GPH['NAME'].tolist()+GPELOC_IREL['NAME'].tolist()\n",
    "len(liste_ent_trouvables)\n",
    "# GPELOC_IREL['NAME'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A l'issue de cette première étude, on en conclut la stratégie d'action suivante:**\n",
    "\n",
    "   1) Trouver les `23 GPELOC` de la base de données `GPH` => Les retirer de la liste de recherche des GPELOC\n",
    "\n",
    "   2) Dans `IREL`, chercher les 103-11=`92 noms` de lieux restant \n",
    "\n",
    "   3) Chercher les 225-126 = `99` entités restantes avec `Geonames`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Retrouver les coordonnées géographiques de chaque base de donnée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. GPH : retrouver par WikiData les coordonnées de tous les lieux \n",
    "=> La méthode que nous avons mis en place pour récupérer l'ensemble des coordonnées n'a pas été concluante malgré les nombreux efforts pour éviter l'ensemble des messages d'erreur provenantde WikiData. Nous envisageons reprendre la fonction du module pour l'appliquer par silots de 100 lieux afin d'avoir plus de contrôle dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwikidata.entity import WikidataItem, WikidataLexeme, WikidataProperty\n",
    "from qwikidata.linked_data_interface import get_entity_dict_from_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"(Q\\d+)\"\n",
    "df_wikiData[\"Q_code\"] = df_wikiData[\"wikidata\"].str.extract(pattern)\n",
    "df_wikiData.to_csv(\"./output_secondaires/GPH_Wikidata_Qcode.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Méthode en question (cf la fonction Extract_longlat_WikiData dans le module) : \n",
    "\n",
    "# pattern = r\"(Q\\d+)\"\n",
    "# df_wikiData[\"Q_code\"] = df_wikiData[\"wikidata\"].str.extract(pattern)\n",
    "# List_Q_code =df_wikiData[\"Q_code\"].tolist()\n",
    "# #Tâche EXTREMEMENT longue.A n'executer qu'une fois : \n",
    "# df_longlatGPH = Extract_longlat_WikiData_all(List_Q_code)\n",
    "# df_longlatGPH.to_csv(\"./GPH_coordonnees.csv\")\n",
    "# df_longlatGPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     Extract_longlat_WikiData_ALL (List_Q_code)\n",
    "# except KeyError :\n",
    "#     print(KeyError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a donc décidé de récupérer uniquement les coordonnées des lieux paraissant dans les revues de 1898 à 1908."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPELOC_GPH =pd.merge(df_wikiData,df_GPELOC,how=\"inner\",on=\"NAME\")[[\"NAME\",\"Q_code\"]]\n",
    "# GPELOC_GPH\n",
    "\n",
    "List_Q_code =GPELOC_GPH[\"Q_code\"].tolist()\n",
    "\n",
    "# df_longlatGPELOC_GPH = Extract_longlat_WikiData(List_Q_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# list_lat=[]\n",
    "# list_long=[]\n",
    "\n",
    "# for Q_code in List_Q_code:\n",
    "# #     print(Q_code)\n",
    "#     if type(Q_code) == float:\n",
    "#         list_lat.append(None)\n",
    "#         list_long.append(None)\n",
    "#     else:\n",
    "#         q_dict = get_entity_dict_from_api(Q_code)\n",
    "\n",
    "\n",
    "#         if \"P625\"in q_dict[\"claims\"] : #Si l'entité a des coordonnées\n",
    "\n",
    "#             #Latitude\n",
    "#             q_latitude = q_dict[\"claims\"][\"P625\"][0][\"mainsnak\"]['datavalue']['value']['latitude']\n",
    "#             list_lat.append(q_latitude)\n",
    "\n",
    "#             #Longitude:\n",
    "#             q_longitude= q_dict[\"claims\"][\"P625\"][0][\"mainsnak\"]['datavalue']['value']['longitude']\n",
    "#             list_long.append(q_longitude)\n",
    "\n",
    "#         else:\n",
    "#             list_lat.append(None)\n",
    "#             list_long.append(None) \n",
    "\n",
    "# #Vérifier la taille des listes pour se prémunir d'un \n",
    "# print(\"Avant correction. \\nliste latitude : \",len(list_lat),\"\\nliste longitude : \",len(list_long),\"\\nList_Q_code : \",len(List_Q_code))\n",
    "\n",
    "# #s'Ily a une différence de taille entre list_long/list_lat et List_Q_code     \n",
    "# # list_long=list_long.append(\"0\")\n",
    "# # list_lat=list_lat.append(\"0\")\n",
    "\n",
    "# #     print(\"Après correction. \\nliste latitude : \",len(list_lat),\"\\nliste longitude : \",len(list_long),\"\\nList_Q_code : \",len(List_Q_code))\n",
    "\n",
    "# # #création d'un df qui rassemble toutes les infos voulues et que l'on pourra merge par Q_code :\n",
    "# # df_longlatGPELOC_GPH=pd.DataFrame({\"Q_code\":List_Q_code,\"latitude\":list_lat,\"longitude\":list_long})\n",
    "\n",
    "# ##Exportation du df : \n",
    "# #   df_longlatGPELOC_GPH.to_csv(\"./GPELOC_GPH_coordonnees.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_longlatGPELOC_GPH=pd.read_csv(\"./output_finaux/GPELOC_GPH_coordonnees.csv\")\n",
    "GPELOC_GPH=pd.merge(GPELOC_GPH,df_longlatGPELOC_GPH,how=\"left\",on=\"Q_code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On merge la liste des latitudes/longitudes `GPELOC_GPH ` (càd trouvées dans la base de données GPH) avec la liste de l'ensemble  des lieux à géoréférencer pour nos revues, `df_GPELOC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GPELOC=pd.merge(df_GPELOC,GPELOC_GPH,how=\"outer\",on=\"NAME\")\n",
    "df_GPELOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. GPH x IREL : avoir une idée de ce que l'on pouvait trouver dans les deux \n",
    "#### (et qu'il faut donc extraire de la liste de recherche)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification que les 11 noms qui étaient trouvables par IREL et par GPH ont bien été trouvés à la section 2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On confirme que GPELOC_IRELGPH n'apporte rien  de nouveau \n",
    "pd.merge(GPELOC_IRELGPH,GPELOC_GPH,how=\"outer\",on=\"NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2 IREL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_IREL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1.reconstituer les URL de IREL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#encoder les titres\n",
    "list_titre_encoded =[]\n",
    "for titre in df_IREL[\"titre\"]:\n",
    "    titre_encoded=urlencode(titre)\n",
    "    list_titre_encoded.append(titre_encoded)\n",
    "\n",
    "df_IREL[\"titre_encoded\"] = list_titre_encoded\n",
    "\n",
    "#reconstituer l'URL\n",
    "list_url=[]\n",
    "for encoded in df_IREL[\"titre_encoded\"]:\n",
    "    IREL_url= \"http://anom.archivesnationales.culture.gouv.fr/geo.php?ir=&lieu=\"+encoded\n",
    "    list_url.append(IREL_url)\n",
    "    \n",
    "df_IREL[\"IREL_url\"] = list_url\n",
    "\n",
    "#Chercher les coordonnées de chaque entrée:\n",
    "#Beautiful Soup\n",
    "pattern_coord_lat = r\"\\\"value\\\"\\:\\{\\\"latitude\\\"\\:(-?[\\d]{1,2}\\.[\\d]*)\\,\"\n",
    "pattern_coord_long = r\"longitude\\\"\\:(-?[\\d]{1,3}\\.[\\d]*)\\,\"\n",
    "\n",
    "df_IREL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Par économie de mémoire et de temps, à l'issue de la récupération des URL nous les avons exporté dans un csv trouvable ici : \"./output_finaux/ListeCoordonnees.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### LISTE des coordonnées récupérée et exportée dans \"./output_finaux/ListeCoordonnees.csv\"\n",
    "\n",
    "# liste_coords=[]\n",
    "# for url in df_IREL[\"IREL_url\"]: \n",
    "#     coords = desambiguisation.Extract_longlat_IREL(url)\n",
    "#     liste_coords.append(coords)\n",
    "# df_IREL[\"Coordonnees\"]=liste_coords\n",
    "# df_IREL[\"Coordonnees\"].to_csv(\"./output_finaux/ListeCoordonnees.csv\",sep='\\t',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IREL[\"Coordonnees\"]=pd.read_csv(\"./output_finaux/ListeCoordonnees.csv\",sep='\\t')\n",
    "\n",
    "#En faisant mon code précédent, j'ai fait une erreur en prenant dans la même colonne\n",
    "#longitude et latitude. \n",
    "df_IREL=pd.concat([df_IREL,df_IREL[\"Coordonnees\"].str.split(\" \",expand=True)],axis=1)\n",
    "df_IREL.rename(columns={0:\"latitude\",1:\"longitude\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Un point sur les coordonnnées géographiques manquantes</h4>\n",
    "\n",
    "On a <b><u>5805</u></b> noms de lieux de la base IREL sans coordonnées géographiques. \n",
    "\n",
    "===>> Question à régler plus tard pour avoir à l'avenir une bdd pertinente et réutilisable par toutes et tous : L'absence de long/lat tient notamment à des problèmes de ponctuation, d'espace et d'appostrophe qui se sont donc mal transcris avec `url_encode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nombre de coordonnées manquantes : \",df_IREL[\"Coordonnees\"].isna().sum())\n",
    "\n",
    "df_IREL.loc[df_IREL[\"Coordonnees\"].isna()][\"titre\"].sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IREL.to_csv(\"./output_finaux/IREL_Coordonnees.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des colonnes Aministration et Lieu-dit du df IREL \n",
    "\n",
    "Les deux colonnes en questions nous permettront de déterminer si oui ou non le lieu est géolocalisé dans la base de donnée IREL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_IREL=IREL_Nettoyage_AdminLieuDit(df_IREL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IREL=df_IREL.rename(columns={0:\"latitude\",1:\"longitude\"})\n",
    "df_IREL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère tous les noms de lieux en liste pour pouvoir comparer avec notre liste de GPELOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#On récupère tous les noms de lieu: \n",
    "liste_colnames= ['Administration', 'NAME','Administration_bis',\n",
    "       'Lieu-dit_bis','latitude','longitude']\n",
    "\n",
    "liste_entites_clean=[]\n",
    "for colname in liste_colnames:\n",
    "    liste_entites_clean+=df_IREL[colname].to_list()\n",
    "    \n",
    "while None in liste_entites_clean:\n",
    "        liste_entites_clean.remove(None)\n",
    "\n",
    "liste_entites_clean\n",
    "\n",
    "#Avoir une liste avec 1seule occurence de nom de lieu :\n",
    "IREL_listeclean = pd.DataFrame(liste_entites_clean).rename(columns={0:\"entites\"})\n",
    "IREL_listeclean = IREL_listeclean.groupby(by=\"entites\").sum().index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IREL_listeclean2=[]\n",
    "for placeName in IREL_listeclean:\n",
    "    \n",
    "    if re.search(r\" $\",placeName):\n",
    "        placeName=placeName.replace(re.search(r\" $\",placeName).group(0),\"\")    \n",
    "\n",
    "    if re.search(r\"^ \",placeName):\n",
    "        placeName=placeName.replace(re.search(r\"^ \",placeName).group(0),\"\")\n",
    "    \n",
    "    IREL_listeclean2.append(placeName)\n",
    "# IREL_listeclean2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chercher quels noms de lieux de la liste de mon df se trouve dans celle des IREL  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on a déjà trouvé certains lieux avec GPH, on retire ceslieux de `list_GPELOC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on créé une nouvelle liste avec les lieux qui n'ont pas encore de long/lat\n",
    "list_GPELOC = list(df_GPELOC.loc[df_GPELOC[\"latitude\"].isna()][\"NAME\"])\n",
    "len(list_GPELOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_MatchDF_IREL, L_NotMatchDF_IREL =MatchGPELOC_IREL(list_GPELOC,IREL_listeclean2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPLORATION DES DEUX LISTES POUR COMPRENDRE CE QUI N'EST PAS PASSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #pour que les deux listes aient la même longueur:\n",
    "# lengthDiff=len(L_NotMatchDF_IREL)-len(L_MatchDF_IREL)\n",
    "# L_MatchDF_IREL += list(np.empty(shape = (lengthDiff)))\n",
    "\n",
    "L_MatchDF_IREL,L_NotMatchDF_IREL=EgaliserTaille_MatchNot_IREL(L_MatchDF_IREL,L_NotMatchDF_IREL)\n",
    "\n",
    "comparaison_matchIREL = pd.DataFrame({'L_MatchDF_IREL' : L_MatchDF_IREL,\n",
    "                                'L_NotMatchDF_IREL' : L_NotMatchDF_IREL},\n",
    "                                columns=['L_MatchDF_IREL','L_NotMatchDF_IREL'])\n",
    "#Pour exploration manuelle\n",
    "comparaison_matchIREL.to_csv(\"./check_comparaisonMatchIREL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionnaire pour la désambiguisation :\n",
    "Dico_desambiguisation = {\"Guinée Française\":[\"Guinée française\"],\n",
    "                         \"Côte d'Ivoire\":[\"côte d'Ivoire\"],\n",
    "                         \"Pak-Hoi\":[\"Pak-Hoï\",\"Pakhoï\"],\n",
    "                         \"Chengdu\":['Tchen-Tou','Tchentou'],\n",
    "                         \"Yunnan\":['Yun-Nam','Yun-Nan','Yunnam'],\n",
    "                         \"Hanoi\":[\"Hanoï\"],\n",
    "                         \"Laokay\":[\"Lao-kay\"],\n",
    "                         \"Côte d\\'Ivoire\":[\"Côte d'Ivoire\",\"côte d'Ivoire\"]\n",
    "                        }\n",
    "Liste_FP = [\"Annamite\",\"Annamites\",\"île\",\"Croyance\",\"Port\"]\n",
    "\n",
    "#Problemes:\n",
    "#Guinée trop flou pour être assigné à \"Guinée française\" car à la même époque existait\n",
    "# une Guinée allemande et une britannique..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nettoyage en fonction de la désambiguisation \n",
    "liste_GPELOC = nettoyage_desambiguisation(Dico_desambiguisation,list_GPELOC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L_MatchDF_IREL2, L_NotMatchDF_IREL2 = MatchGPELOC_IREL(list_GPELOC,IREL_listeclean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pour que les deux listes aient la même longueur:\n",
    "lengthDiff=abs(len(L_NotMatchDF_IREL2)-len(L_MatchDF_IREL2))\n",
    "\n",
    "if len(L_NotMatchDF_IREL2) > len(L_MatchDF_IREL2):\n",
    "    L_MatchDF_IREL2 += list(np.empty(shape = (lengthDiff)))\n",
    "    print(\"nouvelle taille L_Match_IREL2\",len(L_Match_IREL2))\n",
    "    \n",
    "elif len(L_NotMatchDF_IREL2) < len(L_MatchDF_IREL2):\n",
    "    L_NotMatchDF_IREL2 += list(np.empty(shape = (lengthDiff)))\n",
    "    print(\"nouvelle taille L_NotMatch_IREL2\",len(L_NotMatchDF_IREL2))\n",
    "\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Différence de taillent entre les deux listes : \",lengthDiff)\n",
    "print(\"taille de L_NotMatchDF_IREL2 : \",len(L_NotMatchDF_IREL2))\n",
    "print(\"taille de L_MatchDF_IREL2 : \",len(L_MatchDF_IREL2))\n",
    "\n",
    "comparaison_matchIREL = pd.DataFrame({'L_MatchDF_IREL' : L_MatchDF_IREL2,\n",
    "                                'L_NotMatchDF_IREL' : L_NotMatchDF_IREL2},\n",
    "                                columns=['L_MatchDF_IREL','L_NotMatchDF_IREL'])\n",
    "#Pour exploration manuelle\n",
    "comparaison_matchIREL.to_csv(\"./check_comparaisonMatchIREL.csv\")\n",
    "comparaison_matchIREL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pour les entités trouvées : les associer aux df AHMC et AMN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparaison_matchIREL.to_csv(\"./desambiguisation/comparaison_match_IREL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## À titre d'exemple:\n",
    "# df_IREL.loc[df_IREL[\"titre\"].str.contains(\"Brest\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maintenant on cherche à extraire de ` df_IREL ` les coordonnées des lieux qui ont matché "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ensemble = pd.DataFrame()\n",
    "for placeName in L_MatchDF_IREL2:\n",
    "    if type(placeName) == str:\n",
    "        df_placeName=df_IREL.loc[df_IREL[\"titre\"].str.contains(placeName)]\n",
    "        df_placeName[\"placeName\"]=placeName\n",
    "        df_ensemble=pd.concat([df_ensemble,df_placeName])\n",
    "df_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ensemble[\"placeName\"]=df_ensemble[\"placeName\"].drop_duplicates(keep=\"first\")\n",
    "df_ensemble_byPlaceName = df_ensemble.groupby(by=\"placeName\").sum()\n",
    "\n",
    "# df_ensemble_byPlaceName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ensemble_byPlaceName=df_ensemble_byPlaceName.drop(columns=[\"titre_encoded\",\"Administration_bis\",\"Lieu-dit_bis\"])\n",
    "df_ensemble_byPlaceName.to_csv(\"./desambiguisation/IREL_parlieuxreconnus.csv\")\n",
    "\n",
    "#Nécessité de nettoyer IREL_url des localisations multiples. Nous n'en avons pas besoin car il nous suffit d'avoir un point en sont sein \n",
    "df_ensemble_byPlaceName[\"IREL_url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Exploration des lieux contenant leplus de géolocalisations:\n",
    "df_byPlaceName_freq = df_ensemble.groupby(by=\"placeName\").count()[\"IREL_url\"]\n",
    "df_byPlaceName_freq=df_byPlaceName_freq.sort_values(ascending=False)\n",
    "df_byPlaceName_freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Avoir une vision des localités ayant entre 50 et 500 références :\n",
    "print(df_byPlaceName_freq.loc[(df_byPlaceName_freq<500) & (df_byPlaceName_freq>50)])\n",
    "df_byPlaceName_freq.loc[(df_byPlaceName_freq<500) & (df_byPlaceName_freq>50)].plot.bar(figsize=(60,40), fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Si le terme de désignation est aussi \"vague\" que des noms d'entités géographiques à \"grande échelle\" comme \"Guyane\" ou \"Inde\", on se permet de prendre un site internet \"au hasard\" pour récupérer les coordonnées géographiques</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_ensemble_byPlaceName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "liste_urlIREL = list(df_ensemble_byPlaceName[\"IREL_url\"].str.split(\"http\",expand=True)[2])\n",
    "for i in range (len(liste_urlIREL)):\n",
    "    liste_urlIREL[i]=\"http\"+str(liste_urlIREL[i])\n",
    "# \"http\"+liste_urlIREL[3]\n",
    "liste_urlIREL\n",
    "df_ensemble_byPlaceName[\"IREL_url\"]=liste_urlIREL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ensemble_byPlaceName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ensemble_byPlaceName2 = pd.merge(df_ensemble_byPlaceName,df_IREL,how='left',on=\"IREL_url\")\n",
    "df_ensemble_byPlaceName2 = df_ensemble_byPlaceName2[[\"IREL_url\",\"NAME\",\"latitude\",\"longitude\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GPELOC=pd.merge(df_GPELOC,df_ensemble_byPlaceName2,how=\"outer\",on=\"NAME\")\n",
    "df_GPELOC = df_GPELOC.rename(columns={\"longitude_x\":\"longitude_GPH\",\"longitude_y\":\"longitude_IREL\",\"latitude_x\":\"latitude_GPH\",\"latitude_y\":\"latitude_IREL\"})\n",
    "df_GPELOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GPELOC.to_csv(\"./output_finaux/GPELOC_coordonnées_GPH-IREL.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geonames \n",
    "source: https://www.geonames.org/export/web-services.html\n",
    "\n",
    "username= lgrumbach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "URL=\"Chine\"\n",
    "\n",
    "Geonames_url_base =\"http://api.geonames.org/postalCodeLookupJSON?placename=\"+URL+\"&username=lgrumbach\"\n",
    "Geonames_url_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = urlencode(\"Abala, Subdivision (République du Congo)\")\n",
    "url_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 3 : trouver les coordonnées Wikidata GPH\n",
    "## LISTE DES PROPERTIES :  https://www.wikidata.org/wiki/Wikidata:Database_reports/List_of_properties/all \n",
    "\n",
    "## European Colonialism :https://www.wikidata.org/wiki/Wikidata:WikiProject_European_Colonialism\n",
    "\n",
    "## list of colonial empires:https://www.wikidata.org/wiki/Wikidata:WikiProject_European_Colonialism/list_of_colonial_empires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REssource pour seformer à SPARQL sur Wikidata : \n",
    "https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial#SPARQL_basics\n",
    "\n",
    "\n",
    "Ce dont j'ai besoin:\n",
    "SELCT ?coord\n",
    "WHERE\n",
    "{\n",
    "#?coord has coord QID\n",
    "    ?coord wd:QID\n",
    "}\n",
    "\n",
    "\n",
    "Items= wd: /// properties = wdt:.  \n",
    "=> wdt AVANT wd\n",
    "\n",
    "* \";\" = AND\n",
    "* \".\" = fin d'une requête\n",
    "* \",\" = pour ajouter un item aux mêmes propriétés\n",
    "* \"[]\" = Recherche relative.  Inside the brackets, you can specify predicate-object pairs, just like after a ;\n",
    "* \"p\" : points not to the object, but to a statement node.\n",
    "    *  This node then is the subject of other triples: \n",
    "        * ps: (for property statement) points to the statement object,\n",
    "        * pq: (property qualifier) to qualifiers, \n",
    "        * prov:wasDerivedFrom points to reference nodes (which we’ll ignore for now).\n",
    "\n",
    "\n",
    "AJouter : \n",
    "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\". }\n",
    "=> Sert à afficher le label associé à l'entité recherchée\n",
    "\n",
    "**Instances and classes**\n",
    "\n",
    "\"has\" // \"is\" \n",
    "[objet particulier] IS [concept/subclass] \n",
    "=> différencier concept et subclass -> essayer de remplacer \"is\" par \"is a kind of\". SI ça marche, c'est une subclass. Sinon un concept. \n",
    "ex: Gone with the wind \"is a kind of\" film =>  marche pas => film est un concept\n",
    "\n",
    "\n",
    "**Property paths**\n",
    "\n",
    "\n",
    "wdt:P31/wdt:P279/wdt:P279 \n",
    "P31 = nature de l'el\n",
    "P279 =sous-classede [eltavant]\n",
    "\n",
    "wdt:\n",
    "* **P31** = nature del'élt\n",
    "* **P625** = coordonnées géo\n",
    "* **P17**\t= country\n",
    "* **P530**\tdiplomatic relation\n",
    "* rdfs:label \"Mary Wollstonecraft\"@en;\n",
    "* **P945** =\tallegiance\tcountry (or other power) that the person or organization serves\n",
    "* **P1376**\tcapital of\tcountry, state, department, canton or other administrative division of which the municipality is the governmental seat\n",
    "* **P8119**\tHASC\tcodes to represent country subdivisions\n",
    "* **Q133156** colony \n",
    "* **Q161243** dependent territory\n",
    "\n",
    "=> pour éviter de listertoutes lessubclass, on peut écrire \"*\" ou \"+\" comme enregex\n",
    "\n",
    "**Qualifiers**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Globe oordinates**\n",
    "\n",
    "Coordinate text values have value type globecoordinate and property type globe-coordinate.\n",
    "\n",
    "The simple value of the coordinate is the WKT string with the coordinates, with type geo:wktLiteral, e.g.: \"Point(35.3 12.93)\"^^geo:wktLiteral. The order of the coordinates in WKT is longitude, latitude (since format version 0.0.2).\n",
    "\n",
    "The full value has latitude, longitude and precision as double, and the globe as IRI.\n",
    "\n",
    "\n",
    "**Wikibase Entity Id** \n",
    "\n",
    "Wikibase Entity Id values have value type wikibase-entityid and property type wikibase-item.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Faire lien avec les données wikidata de GPH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraire les entity ID de chaque lien wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikidata_extractID(df,ncol):\n",
    "\n",
    "    ListEntityID = []\n",
    "    for i in range (len(df)):\n",
    "        wikiurl = df.iloc[i,ncol]\n",
    "    #     print(type(wikiurl))\n",
    "        if type(wikiurl) is float:\n",
    "            ListEntityID.append(wikiurl)\n",
    "        else:\n",
    "            entityID = re.search(r'[A-Z][0-9]+$',wikiurl).group(0)\n",
    "            ListEntityID.append(entityID)\n",
    "    return ListEntityID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wikidata\n",
    "wikiURL1 = wikidata_extractID(df_WikiQGIS,3)\n",
    "df_WikiQGIS[\"entityID_1\"]=wikiURL1\n",
    "\n",
    "#wikidata_alt1\n",
    "wikiURLalt1 = wikidata_extractID(df_WikiQGIS,4)\n",
    "df_WikiQGIS[\"entityID_alt1\"]=wikiURLalt1\n",
    "\n",
    "#wikidata_alt2\n",
    "wikiURLalt2 = wikidata_extractID(df_WikiQGIS,5)\n",
    "df_WikiQGIS[\"entityID_alt2\"]=wikiURLalt2\n",
    "\n",
    "#NB: il n'y a pas de wikidata_alt3 pour la période concernée\n",
    "df_WikiQGIS.iloc[2,14]\n",
    "df_WikiQGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_WikiQGIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recherche des coordonnées via l'API wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
